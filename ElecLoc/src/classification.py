# Ref: DOI - 10.1007/s11263-011-0474-7

from electrode_models import (ElectrodeModel, LinearElectrodeModel, 
                              ParabolicElectrodeModel)
from utils import log, estimate_intercontact_distance

import numpy as np
from numpy import cross
from numpy.linalg import norm
from typing import List, Tuple, Type


def __get_K_nearest_neighbors(contacts: np.ndarray, k:int) -> List[np.ndarray]:
    """Returns the K closest neighbors of each contact.

    ### Inputs:
    - contacts: an array of shape (N, 3) that contains the coordinates of the 
    input.
    - k: the number of neighbors to include.

    ### Outputs:
    - neighbors: an array of shape (K, N, 3) that contains the absolute
    coordinates of the K closest neighbors.
    """
    # Computing the distance map of the contacts
    # Generated by ChatGPT
    diff = contacts[:, np.newaxis, :] - contacts[np.newaxis, :, :]
    distance_map = np.sqrt(np.sum(diff**2, axis=-1))

    # a value that makes an entry of the distance map unpickable
    UNPICKABLE = distance_map.max()    

    # Sabotaging the diagonal so that a contact cannot be closest to itself
    n = contacts.shape[0]
    distance_map[range(n), range(n)] = UNPICKABLE

    # Computing the N closest contact to each contact
    # closest[i] is the coordinates of the contact that is closest to contact i
    neighbors = []
    for i in range(k):
        closest_indices = np.argmin(distance_map, axis=1)   # Shape (N,)
        neighbors.append(contacts[closest_indices])    # Shape (N, 3)
        distance_map[range(n), closest_indices] = UNPICKABLE

    return np.stack(neighbors)


def __compute_models_distances_matrix(
        models: List[SegmentElectrodeModel]) -> np.ndarray:
    """Computes the distances between each pair of models,
    weighted as a function of their relative angle"""
    global intercontact_dist

    n_models = len(models)
    distances = np.zeros((n_models, n_models), dtype=float)

    M = 5    # Max value of coeff (in range [1, M])
    
    for i, model_i in enumerate(models):
        xi_a, xi_b = model_i.get_segment_nodes()
        distances[i,i] = 0
        for j, model_j in enumerate(models[i+1:]):
            j += i+1    # To align with actual value of j, not the one in enumerate
            xj_a, xj_b = model_j.get_segment_nodes()
            dist_ij = np.min([
                norm(xi_a - xj_a),
                norm(xi_a - xj_b),
                norm(xi_b - xj_a),
                norm(xi_b - xj_b),
            ])
            vi, vj = model_i.direction, model_j.direction
            cosine = np.dot(vi, vj) / (norm(vi) * norm(vj))
            coeff_ij = (M+1)/2 - (M-1)/2 * (2 * cosine**2 - 1)

            #TODO remove debug
            if (intercontact_dist + dist_ij) * coeff_ij == 0:
                debug=0

            distances[i, j] = (intercontact_dist + dist_ij) * coeff_ij
            distances[j, i] = distances[i, j]
    return distances


def __compute_dissimilarity_matrix(
        models: List[ElectrodeModel],
        contacts: np.ndarray,
        labels: np.ndarray
) -> np.ndarray:
    """TODO write documentation"""
    n_models = len(models)
    dissim_matrix = np.empty((n_models, n_models), dtype=np.float64)

    # TODO keep or remove (new)
    # Shape (K, N)
    distances = __compute_points_models_distances(contacts, models).T
    for i, _ in enumerate(models):
        for j, _ in enumerate(models[i:]):
            distances_i_to_j = distances[j][labels == i]
            distances_j_to_i = distances[i][labels == j]
            distances_ij = np.concatenate([distances_i_to_j, distances_j_to_i])
            dissim_ij = np.mean(distances_ij**2) if len(distances_ij) > 0 else -1
            dissim_matrix[i, j] = dissim_ij
            dissim_matrix[j, i] = dissim_ij
    
    # Pairs of models without any sample are very dissimilar by default
    dissim_matrix[dissim_matrix == -1] = dissim_matrix.max()
    
    """TOOD keep or remove (old)
    for i, model_i in enumerate(models):
        for j, model_j in enumerate(models[i:]):
            dissim = model_i.compute_dissimilarity(model_j)
            dissim_matrix[i, j] = dissim
            dissim_matrix[j, i] = dissim
    """

    return dissim_matrix


def __compute_neighborhood_matrix(
        contacts: np.ndarray,
        model_cls: Type[ElectrodeModel],
        c: float=1.0
) -> np.ndarray:
    """TODO write documentation"""
    # The neighbors of each contact. Shape (k, N, 3)
    neighs = __get_K_nearest_neighbors(contacts, model_cls.MIN_SAMPLES)    
    n_contacts = len(contacts)

    # The k-1 closest neighbors of each contact. Shape (k-1, N, 3)
    neighs = __get_K_nearest_neighbors(contacts, model_cls.MIN_SAMPLES-1)

    # For each contact (index 1) the coordinates (index 2) 
    # of itself and its neighbors (index 1). Shape (k, N, 3)
    all_samples = np.concatenate([neighs, contacts[np.newaxis,...]])

    # Associating one model to each contact
    labels = np.arange(n_contacts)
    models = []
    n_contacts = all_samples.shape[1]
    for i in range(n_contacts):
        model = model_cls(all_samples[:,i,:])
        models.append(model)
    return np.exp(- __compute_dissimilarity_matrix(models, contacts, labels) / c**2)

# TODO keep or remove (new)
def __neighbors_models_sampling(
        contacts: np.ndarray,
        model_cls: Type[ElectrodeModel]
) -> List[ElectrodeModel]:
    neighbors = __get_K_nearest_neighbors(contacts, model_cls.MIN_SAMPLES-1)
    # Shape (n_samples_per_model, n_models, 3)   where n_models == n_contacts
    samples = np.concatenate([contacts[np.newaxis,...], neighbors])

    models = []
    for k in range(len(contacts)):
        model = model_cls(samples[:,k])
        models.append(model)
    
    return models

# TODO keep or remove (old)
def __random_models_sampling(
        contacts: np.ndarray, 
        n_models: np.ndarray, 
        model_cls: Type[ElectrodeModel]
) -> List[ElectrodeModel]:
    """TODO write documentation"""
    generator = np.random.default_rng(seed=42)
    # The samples to use for the models. Shape (n_models, k, 3).
    # k is the number of samples needed to generate one model (e.g. one line)
    # 3 is the number of dimensions of each sample (i.e. contact)
    samples = []
    for _ in range(n_models):
        samples.append(
            generator.choice(
                contacts, (model_cls.MIN_SAMPLES,), replace=False, axis=0))
    samples = np.stack(samples)

    models = []
    for k in range(n_models):
        model = model_cls(samples[k])
        models.append(model)
    
    return models



def __compute_points_models_distances(
        contacts: np.ndarray,
        models: List[ElectrodeModel]
) -> np.ndarray:
    """Returns the distance between each point and the given models.
    
    ### Inputs:
    - contacts: the points in 3D space. Shape (N, 3).
    - models: the list of K models (e.g. linear regression, ...).
    
    ### Output:
    - distances: the distance matrix between each contact (point) and each
    model. Shape (N, K)"""

    # Returns distances for one line. Output shape (N,)
    distances = []
    for model in models:
        distances.append(model.compute_distance(contacts))
    return np.stack(distances, axis=-1)


""" TODO remove if useless (old)
def __compute_labels_and_energy(
        contacts: np.ndarray, 
        models: List[ElectrodeModel],
        neighborhood_matrix: np.ndarray,
        lambda_weight: float
) -> float:
    \"""TODO write documentation\"""
    # TODO add outlier system with params = (cost, threshold)

    # Shape (N, K)
    distances = __compute_points_models_distances(contacts, models)
    labels = distances.argmin(axis=1)    # Shape (N,)
    # Sum of distances between each contact and its closest model
    model_cost = distances.min(axis=1).sum()
     
    # Using Potts' model for computing regularization term
    deltas = labels[np.newaxis,:] != labels[:, np.newaxis]

    # Penalty for having similar contacts with different labels
    neighborhood_penalty = np.sum(deltas * neighborhood_matrix)

    tot_energy = model_cost + lambda_weight * neighborhood_penalty
    return labels, tot_energy
"""


def __compute_labels(
        contacts: np.ndarray, 
        models: List[ElectrodeModel]
) -> float:
    """TODO write documentation"""
    # Shape (N, K)
    distances = __compute_points_models_distances(contacts, models)
    labels = distances.argmin(axis=1)    # Shape (N,)
    return labels


def __delete_model(
    models: List[ElectrodeModel],
    index: int | List[int],
    labels: np.ndarray
) -> None:
    """Safely deletes one or several models and ensures that the labels are 
    still matching.
    The labels of the deleted model(s) are replaced by -1."""
    if isinstance(index, (int, np.int64)):
        index = [index]

    # Deleting the indices by decreasing value to avoid conflicts
    index = (sorted(index, reverse=True))

    for i in index:
        # Deleting 'index' and place the last model at that index instead
        models[i] = models[-1]
        del models[-1]

        if i == labels.max():
            labels[labels == i] = -1
        else:
            labels[labels == i] = -1
            labels[labels == labels.max()] = i


def __recompute_models(
        contacts: np.ndarray, 
        models: List[ElectrodeModel], 
        labels: np.ndarray
) -> None:
    """TODO write documentation"""
    
    global intercontact_dist
    c = 2*intercontact_dist

    distances = __compute_points_models_distances(contacts, models)
    for k, model in enumerate(models):
        # The distance between the contacts labelled k and model k
        distances_k = distances[labels == k][:,k]
        weights = np.exp(-distances_k**2 / c)
        inliers = contacts[labels == k]    # Shape (NB_INLIERS, 3)
        model.recompute(inliers, weights)


def __merge_two_most_similar_models(
        models: List[ElectrodeModel], 
        labels: np.ndarray, 
        contacts: np.ndarray,
        n_electrodes: int
) -> Tuple[List[ElectrodeModel], np.ndarray]:
    """TODO write documentation"""

    ### Merging similar models
    if len(models) <= n_electrodes:
        return models, labels

    # Computing dissimilarity between all pairs of models (ignoring diagonal)
    n_models = len(models)
    # TODO keep or delete
    #dissim_scores = __compute_dissimilarity_matrix(models, contacts, labels)
    dissim_scores = __compute_models_distances_matrix(models)

    dissim_scores[range(n_models),range(n_models)] = dissim_scores.max()

    # Selecting a pair of models to merge, and the size of their support
    i, j = np.unravel_index(dissim_scores.argmin(), dissim_scores.shape)

    # Deleting the two models, and recomputing a new one
    # from the orphan contacts
    model_cls = models[i].__class__
    __delete_model(models, [i, j], labels)
    models.append(model_cls(contacts[labels == -1]))
    labels[labels == -1] = len(models)-1    # useless

    labels = __compute_labels(contacts, models)
    __recompute_models(contacts, models, labels)

    return models, labels


def __delete_unsupported_models(
        models: List[ElectrodeModel],
        labels: np.ndarray,
        contacts: np.ndarray,
        n_electrodes: int
) -> Tuple[List[ElectrodeModel], np.ndarray]:
    """TODO write documentation"""
    counts = np.bincount(labels, minlength=len(models))    # Shape (N_MODELS,)
    for k in range(len(models)-1, -1, -1):
        if (counts[k] < 1 # models[k].MIN_SAMPLES  # TODO test uncomment
                and len(models) > n_electrodes):
            __delete_model(models, k, labels)
    labels = __compute_labels(contacts, models)
    __recompute_models(contacts, models, labels)
    return models, labels


def __delete_most_redundant_model(
        models: List[ElectrodeModel],
        labels: np.ndarray,
        contacts: np.ndarray,
        n_electrodes: int
) -> Tuple[List[ElectrodeModel], np.ndarray]:
    """Deletes the most redundant model, i.e. the model k with smallest average 
    distance to contacts which have k as their second closest model."""
    if len(models) <= n_electrodes:
        return models, labels
    
    # Computing, for each contact, the index of the second closest model
    distances = __compute_points_models_distances(contacts, models)
    distances[range(len(contacts)), distances.argmin(axis=1)] = distances.max()
    idx_second = distances.argmin(axis=1)        # Shape (N,)

    scores = []
    for k, _ in enumerate(models):
        # For each model k, computing the average distance between k
        # and the contacts which have k as second closest model
        inliers_dist_second = idx_second[labels == k]
        if len(inliers_dist_second) > 0:
            scores.append(inliers_dist_second.mean())
        else: 
            # Model k is not the second closest to any contact => ignore
            scores.append(-1)

    # Invalidate ignored models
    scores = np.array(scores)
    scores[scores == -1] = scores.max()

    # Deletes the most redundant model
    deleted_k = np.argmin(scores)
    __delete_model(models, deleted_k, labels)
    labels = __compute_labels(contacts, models)
    __recompute_models(contacts, models, labels)
    return models, labels


def __delete_least_supported_model(
        models: List[ElectrodeModel],
        labels: np.ndarray,
        contacts: np.ndarray,
        n_electrodes: int
) -> Tuple[List[ElectrodeModel], np.ndarray]:
    """Deletes the model with least support in 'labels'."""
    if len(models) <= n_electrodes:
        return models, labels

    counts = np.bincount(labels, minlength=len(models))    # Shape (N_MODELS,)
    deleted_k = np.argmin(counts)
    __delete_model(models, deleted_k, labels)
    labels = __compute_labels(contacts, models)
    __recompute_models(contacts, models, labels)
    return models, labels


def classify_electrodes(
        contacts: np.ndarray,
        n_electrodes: int,
        model_cls: Type[ElectrodeModel] = LinearElectrodeModel
) -> np.ndarray:
    """TODO write documentation"""
    max_iter = 1000000

    global intercontact_dist
    intercontact_dist = estimate_intercontact_distance(contacts)

    # Proposing initial models
    models = __neighbors_models_sampling(contacts, model_cls)

    # Assigning each contact to one model and computing the resulting energy
    labels = __compute_labels(contacts, models)

    models, labels = __delete_unsupported_models(
        models, labels, contacts, n_electrodes)
    
    iter = 0
    previous_labels = np.empty_like(labels)
    # TODO could happen that init >= max_init AND models > n_electrodes, fix that
    # TODO: ensure that energy is decreasing
    while ((not np.all(labels == previous_labels)
           or len(models) > n_electrodes)
           and iter < max_iter):
        iter += 1
        previous_labels[:] = labels[:]

        # Refining the models based on their spatial support
        # TODO keep or delete
        #__recompute_models(contacts, models, labels)

        """models, labels = __delete_unsupported_models(
            models, labels, contacts, n_electrodes)

        models, labels = __merge_two_most_similar_models(
            models, labels, contacts, n_electrodes)
        """models, labels = __delete_most_redundant_model(
            models, labels, contacts, n_electrodes)
        models, labels = __delete_least_supported_model(
            models, labels, contacts, n_electrodes)"""
        
        # TODO remove debug
        log(f"Remaining models: {len(models):<5}", erase=True)

    if (iter == max_iter):
        raise RuntimeWarning("Max iteration reached while computing models.")
        
    return labels, models
